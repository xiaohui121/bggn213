---
title: "Class 8: Machine learning"
author: "Xiaohui Lyu"
date: "2019/4/26"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Kmeans clustering

Let's try out the **kmeans()** function in R with some makieup data

```{r}
# Generate some example data for clustering
tmp <- c(rnorm(30,-3), rnorm(30,3))
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```

Use the kmeans() function setting k to 2 and nstart=20

```{r}
y <- kmeans(x,centers = 2,nstart = 20)
```

Inspect/print the results

> Q. How many points are in each cluster?

> Q. What ‘component’ of your result object details
- cluster size?
- cluster assignment/membership?
- cluster center?

```{r}
y$cluster
table(y$cluster)
y$centers
```

Plot x colored by the kmeans cluster assignment and
add cluster centers as blue points

```{r}
plot(x,col= y$cluster)
points(y$centers, col="blue", pch=19, cex=2)
```

## Hierarchical Clustering

Here we don't have to spell out K the number of clusters before hand but we do have to give it a distance matrix as input.

```{r}
# First we need to calculate point (dis)similarity
# as the Euclidean distance between observations
dist_matrix <- dist(x)
# The hclust() function returns a hierarchical
# clustering model
hc <- hclust(d = dist_matrix)
# the print method is not so useful here
hc
```

Let's plot the results

```{r}
plot(hc)
abline(h=6,col="red")
cutree(hc,h=6)
```

```{r}
gp2 <- cutree(hc, k=2)
gp2
```

```{r}
gp3 <- cutree(hc, k=3)
gp3
```

```{r}
table(gp3)
table(gp2)
table(gp2,gp3)
```

Try a more real-life like example to see how our clustering works

```{r}
# Step 1. Generate some example data for clustering
x <- rbind(
matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
# (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

> Q. Use the dist(), hclust(), plot() and cutree()
functions to return 2 and 3 clusters

> Q. How does this compare to your known 'col' groups?

```{r}
hc1 <- hclust(dist(x))
plot(hc1)
cut2 <- cutree(hc1, k=2)
plot(x,col=cut2)
cut3 <- cutree(hc1,k=3)
plot(x,col=cut3)
table(cut2,cut3)
```

## Principal Component Analysis (PCA)

We will use the base R **prcomp()** function for PCA today.

Let's get som RNAseq data to play with.

```{r}
mydata <- read.csv("expression.csv",row.names=1)
head(mydata)
```

There are `r nrow(mydata)` genes in this dataset

```{r}
## lets do PCA
pca <- prcomp(t(mydata), scale=TRUE)
summary(pca)
attributes(pca)
```

Let's make our first PCA plot

```{r}
## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
colvec <- c(rep("red",5),rep("blue",5))
xlab <- paste("PC1 (",pca.var.per[1],"%)",sep = "")
ylab <- paste("PC2(",pca.var.per[2],"%)",sep = "")

plot(pca$x[,1],pca$x[,2],xlab =xlab, ylab =ylab, col=colvec)
text(pca$x[,1],pca$x[,2],labels=colnames(mydata))
```

## PCA on the UK foods dataset

```{r}
uk <- read.csv("UK_foods.csv",row.names = 1)
uk
```

```{r}
pca_uk <- prcomp(t(uk))
attributes(pca_uk)
uk.var <- pca_uk$sdev^2
uk.var.per <- round(uk.var/sum(uk.var)*100, 1)
barplot(uk.var.per,names.arg = c("PC1","PC2","PC3","PC4"))
xlab <- paste("PC1(",uk.var.per[1],"%)")
ylab <- paste("PC2(",uk.var.per[2],"%)")
mycol <- c("orange","red","blue","dark green")
plot(pca_uk$x[,1],pca_uk$x[,2],xlab=xlab,ylab=ylab,col=mycol,pch=16,cex=2)
text(pca_uk$x[,1],pca_uk$x[,2],labels = colnames(uk))
```
